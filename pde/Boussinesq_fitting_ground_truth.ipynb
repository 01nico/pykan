{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_WIDTH = 30\n",
    "DEFAULT_DEPTH = 7\n",
    "\n",
    "class PINN_f(nn.Module):\n",
    "    def __init__(self, width=DEFAULT_WIDTH, depth=DEFAULT_DEPTH):\n",
    "        super(PINN_f, self).__init__()\n",
    "        self.width = width\n",
    "        if depth < 2:\n",
    "          raise ValueError(f\"depth must be at least 2, got {depth}\")\n",
    "        self.depth = depth\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "        # layer definitions\n",
    "        self.FC_dict = nn.ModuleDict()\n",
    "        self.FC_dict[f\"layer 1\"] = nn.Linear(2, self.width)\n",
    "        for i in range(2, depth):\n",
    "          self.FC_dict[f\"layer {i}\"] = nn.Linear(self.width, self.width)\n",
    "        self.FC_dict[f\"layer {depth}\"] = nn.Linear(self.width, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "      for key, layer in self.FC_dict.items():\n",
    "        x = layer(x)\n",
    "        if key != f\"layer {self.depth}\":\n",
    "          x = self.act(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN_f(\n",
      "  (act): Tanh()\n",
      "  (FC_dict): ModuleDict(\n",
      "    (layer 1): Linear(in_features=2, out_features=30, bias=True)\n",
      "    (layer 2): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (layer 3): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (layer 4): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (layer 5): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (layer 6): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (layer 7): Linear(in_features=30, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_phi = PINN_f().to(device)\n",
    "model_psi = PINN_f().to(device)\n",
    "model_w = PINN_f().to(device)\n",
    "model_u1 = PINN_f().to(device)\n",
    "model_u2 = PINN_f().to(device)\n",
    "\n",
    "print(model_u1)\n",
    "d_INIT = 3\n",
    "l = torch.tensor([d_INIT,], dtype=torch.float64, device=device, requires_grad=True)\n",
    "# define an optimizer\n",
    "params = list(model_psi.parameters()) + list(model_w.parameters())+ list(model_phi.parameters()) + list(model_u1.parameters())+list(model_u2.parameters())\n",
    "\n",
    "\n",
    "nn_optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=0)\n",
    "#nn_scheduler = torch.optim.lr_scheduler.StepLR(nn_optimizer, step_size=1000, gamma=0.5)\n",
    "l_optimizer = torch.optim.Adam([l], lr=0.001, weight_decay=0) #l learning rate can be even better\n",
    "#l_scheduler = torch.optim.lr_scheduler.StepLR(l_optimizer, step_size=1000, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0,  function loss is 0.0008530763867466242, boundary loss is 0.017512156595954995, smoothness loss is 7.897908582589887e-05, overall loss is 0.017605362143212247 \n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_95057/3830135890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# update the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mnn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m#nn_scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# to verify that we have a small loss using approx solutions\n",
    "l=10\n",
    "alpha=1/(1+l)\n",
    "c_norm=24/13/np.pi\n",
    "batchsize = 5000 #interior\n",
    "batchsize1 = 500 #boundary, smoothness? (close to the origin)\n",
    "for ep in range(100000):\n",
    "    nn_optimizer.zero_grad()\n",
    "\n",
    "    #l_optimizer.zero_grad()\n",
    "\n",
    "    ### equation loss\n",
    "    # sample points uniformly in the transformed coordinates [-30,30]x[0,30]\n",
    "    c = 30\n",
    "    x1 = c*torch.rand(batchsize, 1, device=device)# (batch, 1)\n",
    "    x1.requires_grad=True\n",
    "    x2 = c*torch.rand(batchsize, 1, device=device) # (batch, 1)\n",
    "    x2.requires_grad=True\n",
    "    input1 = torch.cat([x1,x2], dim=-1) # (batch, 2)\n",
    "    input2 = torch.cat([-x1,x2], dim=-1) # (batch, 2)\n",
    "    z1=torch.sinh(x1)\n",
    "    z2=torch.sinh(x2)\n",
    "    gamma=torch.atan(z2/z1)\n",
    "    R=(z1**2+z2**2)**(alpha/2)\n",
    "    r=(z1**2+z2**2)**(1/2)\n",
    "    w=-alpha/c_norm*torch.cos(gamma)**alpha*3*R/(1+R)**2\n",
    "    phi=-alpha/c_norm*torch.cos(gamma)**alpha*6*R/(1+R)**3\n",
    "    psi=0*r\n",
    "    u1=-3*z1/(1+R)#z1\n",
    "    u2=3*z2/(1+R)#z2\n",
    "    \n",
    "    \n",
    "    #compute the derivatives (only first orders)\n",
    "    w_1 = grad(w.sum(), x1, create_graph=True)[0]\n",
    "    w_2 = grad(w.sum(), x2, create_graph=True)[0]\n",
    "    phi_1 = grad(phi.sum(), x1, create_graph=True)[0]\n",
    "    phi_2 = grad(phi.sum(), x2, create_graph=True)[0]\n",
    "    psi_1 = grad(psi.sum(), x1, create_graph=True)[0]\n",
    "    psi_2 = grad(psi.sum(), x2, create_graph=True)[0]\n",
    "    u1_1 = grad(u1.sum(), x1, create_graph=True)[0]\n",
    "    u1_2 = grad(u1.sum(), x2, create_graph=True)[0]\n",
    "    u2_1 = grad(u2.sum(), x1, create_graph=True)[0]\n",
    "    u2_2 = grad(u2.sum(), x2, create_graph=True)[0]\n",
    "    \n",
    "    #compute the equation residue\n",
    "    N_e = 6\n",
    "    f_1 = w+((1+l)*torch.sinh(x1)+u1)/torch.cosh(x1)*w_1+((1+l)*torch.sinh(x2)+u2)/torch.cosh(x2)*w_2-phi\n",
    "    f_2 = (2+u1_1/torch.cosh(x1))*phi+((1+l)*torch.sinh(x1)+u1)/torch.cosh(x1)*phi_1+((1+l)*torch.sinh(x2)+u2)/torch.cosh(x2)*phi_2+u2_1/torch.cosh(x1)*psi\n",
    "    f_3 = (2+u2_2/torch.cosh(x2))*psi+((1+l)*torch.sinh(x1)+u1)/torch.cosh(x1)*psi_1+((1+l)*torch.sinh(x2)+u2)/torch.cosh(x2)*psi_2+u1_2/torch.cosh(x2)*phi\n",
    "    f_4 = u1_1/torch.cosh(x1)+u2_2/torch.cosh(x2)\n",
    "    f_5 = w-(u1_2/torch.cosh(x2)-u2_1/torch.cosh(x1))\n",
    "    f_6 = phi_2/torch.cosh(x2)-psi_1/torch.cosh(x1)\n",
    "    \n",
    "    loss_i_1=torch.norm(f_1)**2/batchsize\n",
    "    loss_i_2=torch.norm(f_2)**2/batchsize\n",
    "    loss_i_3=torch.norm(f_3)**2/batchsize\n",
    "    loss_i_4=torch.norm(f_4)**2/batchsize\n",
    "    loss_i_5=torch.norm(f_5)**2/batchsize\n",
    "    loss_i_6=torch.norm(f_6)**2/batchsize\n",
    "    \n",
    "    loss_i = (loss_i_1+loss_i_2+loss_i_3+loss_i_4+loss_i_5+loss_i_6)/N_e\n",
    "    \n",
    "    ###boundary loss\n",
    "    # sample points\n",
    "    y1 = c*(torch.rand(batchsize1, 1, device=device))# (batch1, 1)\n",
    "    y1.requires_grad=True\n",
    "    y2 = c*torch.rand(batchsize1, 1, device=device) # (batch1, 1)\n",
    "    y2.requires_grad=True\n",
    "    y0 = torch.zeros(batchsize1,1, device=device,requires_grad=True)\n",
    "    yd = c*torch.ones(batchsize1,1, device=device)\n",
    "    yd.requires_grad=True\n",
    "    \n",
    "    #boundary 1\n",
    "    input1_b1 = torch.cat([y1,y0], dim=-1) # (batch1, 2)\n",
    "    input2_b1 = torch.cat([-y1,y0], dim=-1) # (batch1, 2)\n",
    "    \n",
    "    z1=torch.sinh(y1)\n",
    "    z2=torch.sinh(y0)\n",
    "    gamma=torch.atan(z2/z1)\n",
    "    R=(z1**2+z2**2)**(alpha/2)\n",
    "    u2_b1=3*z2/(1+R)#z2\n",
    "    b_1 = u2_b1\n",
    "    loss_b_1 = torch.norm(b_1)**2/batchsize1\n",
    "    \n",
    "    \n",
    "    #boundary 3,5,7\n",
    "    input1_b3 = torch.cat([yd,y2], dim=-1) # (batch1, 2)\n",
    "    input2_b3 = torch.cat([-yd,y2], dim=-1) # (batch1, 2)\n",
    "    \n",
    "    z1=torch.sinh(yd)\n",
    "    z2=torch.sinh(y2)\n",
    "    gamma=torch.atan(z2/z1)\n",
    "    R=(z1**2+z2**2)**(alpha/2)\n",
    "    phi_b3=-alpha/c_norm*torch.cos(gamma)**alpha*6*R/(1+R)**3\n",
    "    psi_b3=0*R\n",
    "    u1_b3=-3*z1/(1+R)#z1\n",
    "    u2_b3=3*z2/(1+R)#z2\n",
    "    u1_1_b3 = grad(u1_b3.sum(), yd, create_graph=True)[0]\n",
    "    u1_2_b3 = grad(u1_b3.sum(), y2, create_graph=True)[0]\n",
    "    u2_1_b3 = grad(u2_b3.sum(), yd, create_graph=True)[0]\n",
    "    u2_2_b3 = grad(u2_b3.sum(), y2, create_graph=True)[0]\n",
    "    b_31=u1_1_b3/torch.cosh(yd)\n",
    "    b_33=u1_2_b3/torch.cosh(y2)\n",
    "    b_34=u2_1_b3/torch.cosh(yd)\n",
    "    b_32=u2_2_b3/torch.cosh(y2)\n",
    "    b_5=phi_b3\n",
    "    b_7=psi_b3\n",
    "    loss_b_3 = (torch.norm(b_31)**2+torch.norm(b_32)**2+torch.norm(b_33)**2+torch.norm(b_34)**2+torch.norm(b_5)**2+torch.norm(b_7)**2)/batchsize1\n",
    "    \n",
    "    #boundary 4,6,8\n",
    "    input1_b4 = torch.cat([y1,yd], dim=-1) # (batch1, 2)\n",
    "    input2_b4 = torch.cat([-y1,yd], dim=-1) # (batch1, 2)\n",
    "    \n",
    "    z1=torch.sinh(y1)\n",
    "    z2=torch.sinh(yd)\n",
    "    gamma=torch.atan(z2/z1)\n",
    "    R=(z1**2+z2**2)**(alpha/2)\n",
    "    phi_b4=-alpha/c_norm*torch.cos(gamma)**alpha*6*R/(1+R)**3\n",
    "    psi_b4=0*R\n",
    "    u1_b4=-3*z1/(1+R)#z1\n",
    "    u2_b4=3*z2/(1+R)#z2\n",
    "    u1_1_b4 = grad(u1_b4.sum(), y1, create_graph=True)[0]\n",
    "    u1_2_b4 = grad(u1_b4.sum(), yd, create_graph=True)[0]\n",
    "    u2_1_b4 = grad(u2_b4.sum(), y1, create_graph=True)[0]\n",
    "    u2_2_b4 = grad(u2_b4.sum(), yd, create_graph=True)[0]\n",
    "    b_41=u1_1_b4/torch.cosh(y1)\n",
    "    b_43=u1_2_b4/torch.cosh(yd)\n",
    "    b_44=u2_1_b4/torch.cosh(y1)\n",
    "    b_42=u2_2_b4/torch.cosh(yd)\n",
    "    b_6=phi_b4\n",
    "    b_8=psi_b4\n",
    "    loss_b_4 = (torch.norm(b_41)**2+torch.norm(b_42)**2+torch.norm(b_43)**2+torch.norm(b_44)**2+torch.norm(b_6)**2+torch.norm(b_8)**2)/batchsize1\n",
    "    \n",
    "    #collect the residue\n",
    "    N_b=8\n",
    "    loss_b = (loss_b_1+loss_b_3+loss_b_4)/N_b\n",
    "    \n",
    "    ### smoothness loss\n",
    "    # sample points uniformly in the transformed coordinates [-1,1]x[0,1]\n",
    "   \n",
    "    x1 = c*torch.rand(batchsize1, 1, device=device)# (batch, 1)\n",
    "    x1.requires_grad=True\n",
    "    x2 = c*torch.rand(batchsize1, 1, device=device) # (batch, 1)\n",
    "    x2.requires_grad=True\n",
    "    input1 = torch.cat([x1,x2], dim=-1) # (batch, 2)\n",
    "    input2 = torch.cat([-x1,x2], dim=-1) # (batch, 2)\n",
    "    z1=torch.sinh(x1)\n",
    "    z2=torch.sinh(x2)\n",
    "    gamma=torch.atan(z2/z1)\n",
    "    R=(z1**2+z2**2)**(alpha/2)\n",
    "    r=(z1**2+z2**2)**(1/2)\n",
    "    w=-alpha/c_norm*torch.cos(gamma)**alpha*3*R/(1+R)**2\n",
    "    phi=-alpha/c_norm*torch.cos(gamma)**alpha*6*R/(1+R)**3\n",
    "    psi=0*r\n",
    "    u1=-3*z1/(1+R)#z1\n",
    "    u2=3*z2/(1+R)#z2\n",
    "    \n",
    "    \n",
    "    #compute the derivatives (only first orders)\n",
    "    w_1 = grad(w.sum(), x1, create_graph=True)[0]\n",
    "    w_2 = grad(w.sum(), x2, create_graph=True)[0]\n",
    "    phi_1 = grad(phi.sum(), x1, create_graph=True)[0]\n",
    "    phi_2 = grad(phi.sum(), x2, create_graph=True)[0]\n",
    "    psi_1 = grad(psi.sum(), x1, create_graph=True)[0]\n",
    "    psi_2 = grad(psi.sum(), x2, create_graph=True)[0]\n",
    "    u1_1 = grad(u1.sum(), x1, create_graph=True)[0]\n",
    "    u1_2 = grad(u1.sum(), x2, create_graph=True)[0]\n",
    "    u2_1 = grad(u2.sum(), x1, create_graph=True)[0]\n",
    "    u2_2 = grad(u2.sum(), x2, create_graph=True)[0]\n",
    "    \n",
    "    #compute the equation residue\n",
    "    N_e = 6\n",
    "    f_1 = w+((1+l)*torch.sinh(x1)+u1)/torch.cosh(x1)*w_1+((1+l)*torch.sinh(x2)+u2)/torch.cosh(x2)*w_2-phi\n",
    "    f_2 = (2+u1_1/torch.cosh(x1))*phi+((1+l)*torch.sinh(x1)+u1)/torch.cosh(x1)*phi_1+((1+l)*torch.sinh(x2)+u2)/torch.cosh(x2)*phi_2+u2_1/torch.cosh(x1)*psi\n",
    "    f_3 = (2+u2_2/torch.cosh(x2))*psi+((1+l)*torch.sinh(x1)+u1)/torch.cosh(x1)*psi_1+((1+l)*torch.sinh(x2)+u2)/torch.cosh(x2)*psi_2+u1_2/torch.cosh(x2)*phi\n",
    "    f_4 = u1_1/torch.cosh(x1)+u2_2/torch.cosh(x2)\n",
    "    f_5 = w-(u1_2/torch.cosh(x2)-u2_1/torch.cosh(x1))\n",
    "    f_6 = phi_2/torch.cosh(x2)-psi_1/torch.cosh(x1)\n",
    "    \n",
    "\n",
    "    \n",
    "    #compute the residue derivatives\n",
    "    F_1_1=grad(f_1.sum(), x1, create_graph=True)[0]\n",
    "    F_1_2=grad(f_1.sum(), x2, create_graph=True)[0]\n",
    "    F_2_1=grad(f_2.sum(), x1, create_graph=True)[0]\n",
    "    F_2_2=grad(f_2.sum(), x2, create_graph=True)[0]\n",
    "    F_3_1=grad(f_3.sum(), x1, create_graph=True)[0]\n",
    "    F_3_2=grad(f_3.sum(), x2, create_graph=True)[0]\n",
    "    F_4_1=grad(f_4.sum(), x1, create_graph=True)[0]\n",
    "    F_4_2=grad(f_4.sum(), x2, create_graph=True)[0]\n",
    "    F_5_1=grad(f_5.sum(), x1, create_graph=True)[0]\n",
    "    F_5_2=grad(f_5.sum(), x2, create_graph=True)[0]\n",
    "    F_6_1=grad(f_6.sum(), x1, create_graph=True)[0]\n",
    "    F_6_2=grad(f_6.sum(), x2, create_graph=True)[0]\n",
    "    \n",
    "\n",
    "    loss_s_1=(torch.norm(F_1_1)**2+torch.norm(F_1_2)**2)/batchsize1\n",
    "    loss_s_2=(torch.norm(F_2_1)**2+torch.norm(F_2_2)**2)/batchsize1\n",
    "    loss_s_3=(torch.norm(F_3_1)**2+torch.norm(F_3_2)**2)/batchsize1\n",
    "    loss_s_4=(torch.norm(F_4_1)**2+torch.norm(F_4_2)**2)/batchsize1\n",
    "    loss_s_5=(torch.norm(F_5_1)**2+torch.norm(F_5_2)**2)/batchsize1\n",
    "    loss_s_6=(torch.norm(F_6_1)**2+torch.norm(F_6_2)**2)/batchsize1\n",
    "    \n",
    "\n",
    "    loss_s = (loss_s_1+loss_s_2+loss_s_3+loss_s_4+loss_s_5+loss_s_6)/N_e\n",
    "    \n",
    "    #the total loss\n",
    "    gamma=0.1\n",
    "    loss =gamma*(loss_i+loss_s)+1*loss_b\n",
    "\n",
    "    # update the model\n",
    "    loss.backward()\n",
    "    nn_optimizer.step()\n",
    "    #nn_scheduler.step()\n",
    "\n",
    "    #l_optimizer.step()\n",
    "    #l_scheduler.step()\n",
    "\n",
    "    # plot\n",
    "    if ep % 1000 == 0:\n",
    "        print(\"Epoch is {},  function loss is {}, boundary loss is {}, smoothness loss is {}, overall loss is {} \".format(ep, loss_i.item(),loss_b.item(),loss_s.item(), loss.item()))\n",
    "        #print(ep, 'loss' +{loss_i.item()}, loss_b.item(),loss1.item(),loss2.item(), loss.item())\n",
    "        print(l)\n",
    "        # uniformly sample from [r,z] \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2788],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2834],\n",
       "        [1.2839],\n",
       "        [1.2820],\n",
       "        [1.2839],\n",
       "        [1.2786],\n",
       "        [1.2827],\n",
       "        [1.2785],\n",
       "        [1.2839],\n",
       "        [1.2821],\n",
       "        [1.2835],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2837],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2837],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2820],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2801],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2837],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2823],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2828],\n",
       "        [1.2829],\n",
       "        [1.2832],\n",
       "        [1.2787],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2837],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2836],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2790],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2831],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2836],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2837],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2820],\n",
       "        [1.2836],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2829],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2837],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2835],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2797],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2836],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2779],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2806],\n",
       "        [1.2833],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2834],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2817],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2821],\n",
       "        [1.2838],\n",
       "        [1.2820],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2833],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2812],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2803],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2835],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2835],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2826],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2823],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2838],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839],\n",
       "        [1.2839]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
